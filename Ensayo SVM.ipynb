{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Nombre:** Elda Magally Calderón Motta\n",
    "### **Carné:** 16003182\n",
    "### **Sección:** Virtual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Elda\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "if tf.__version__.startswith(\"2.\"):\n",
    "  import tensorflow.compat.v1 as tf\n",
    "  tf.compat.v1.disable_v2_behavior()\n",
    "  tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensayo Support Vector Machine - SVM -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM es un algoritmo de Machine Learning de aprendizaje supervisado que puede usarse tanto en regresión como clasificación, aunque su mayor enfoque es este último caso, incluso puede utilizarse para regresión no lineal y detección de anomalías.\n",
    "\n",
    "Este algoritmo construye un hiperplano óptimo en forma de superficie de decisión, de modo que el margen de separación entre las dos clases en los datos se amplía al máximo. \n",
    "\n",
    "Las máquinas de vectores de soporte son una técnica de machine learning que encuentra la mejor separación posible entre clases. En lugar de encontrar la línea óptima, el SVM encuentra el hiperplano que maximiza el margen de separación entre clases.\n",
    "\n",
    "## Vectores de Soporte:\n",
    "\n",
    "Los vectores de soporte son los puntos que definen el margen máximo de separación del hiperplano que separa las clases, como la analogía que vimos en clase, estos puntos definirían las orillas de la calle que \"atraviesa\" el modelo entre las clases. \n",
    "\n",
    "Algo interesante que encontré en la documentación es el hecho de porque se le llaman vectores, en lugar de puntos, esto es porque estos \"puntos\" tienen tantos elementos como dimensiones tenga el espacio de entrada. Es decir, estos puntos multi-dimensionales se representan con un vector de n dimensiones.\n",
    "\n",
    "En la imagen siguiente se ilustra como trabaja este algoritmo para clasificación\n",
    "\n",
    "<img src=\"https://iartificial.net/wp-content/uploads/2019/04/Clasificacion-SVM-768x436.png\"/>\n",
    "\n",
    "El algoritmo trabaja de tal forma que se establece una recta de separación de las clases, luego se establece un margen que son los vectores más cercanos a la recta de separación, los cuales son llamados vectores de soporte.\n",
    "El margen es la distancia existente entre las rectas punteadas (que son las definidas por los vectores de soporte), por lo que el algoritmo de VSM busca maximizar este margen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proceso de Entrenamiento\n",
    "\n",
    "Este algoritmo puede ser empleado tanto para variables categóricas como cuantitativas. Si se trabaja con variables categóricas es necesario realizar un proceso de feature engineering tal como one-hot-encoding.\n",
    "\n",
    "Luego como cualquier modelo de machine learning requiere hacer un análisis exploratorio de datos para intentar identificar el comportamiento y facilitar la elección del kernel.\n",
    "\n",
    "### Kernel Trick:\n",
    "\n",
    "Es una técnica empleada en conjunto con el algoritmo de SVM para generar nuevas features. De esta forma, si hay datos que no se puedan clasificar de forma lineal se puede emplear esta técnica para que el algoritmo puede realizar la clasificación. Un kernel es una función que mapea las features existentes a un nuevo espacio con diferentes dimensiones. \n",
    "\n",
    "https://www.youtube.com/watch?v=xflpbarqi1I&feature=youtu.be\n",
    "\n",
    "Un conjunto de datos que pertenzcan a una dimensión $d$ pueden ser linealmente separables si existe un hiperplano de decisión $f(x)$ definido de la siguiente manera:\n",
    "\n",
    "$f(x) = w^{T}x + w_0$\n",
    "\n",
    "Donde $w^{T}$ es el vector de pesos y $w_0$ es el bias.\n",
    "\n",
    "Esta dimensión $d$ refleja la forma del hyperplano. Según la dimensión en que se encuentren los datos puede resultar fácil o difícil separarlos linealmente, por lo que el truco de kernel nos puede ayudar para esto.\n",
    "\n",
    "\n",
    "Hay diferentes tipos de kernel que pueden ser aplicados cuando tenemos datos que no pueden ser separados por una línea en un único plano. A continuación se presentan las funciones para algunos kernel, tomados como referencia del siguiente video:\n",
    "\n",
    "https://www.youtube.com/watch?v=VWwb3IAB6Rc&feature=youtu.be\n",
    "\n",
    "* **Lineal** \n",
    "\n",
    "$K(x,x') = x\\cdot x' + 1$\n",
    "\n",
    "* **Polinómico**\n",
    "\n",
    "$K(x,x') = (x\\cdot x' + 1)^{k}$\n",
    "\n",
    "* **Gausiano (RBF)**\n",
    "\n",
    "$K(x,x') = e\\left(\\frac{-||x-x'||^{2}}{2\\sigma^{2}}\\right)$\n",
    "\n",
    "La selección del kernel va a depender de la naturaleza de los datos, en como se comporten. Un ejemplo de como actúan los kernel se muestra en las imagenes siguientes:\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Katarzyna_Matek/publication/235892711/figure/fig1/AS:299920994127876@1448518145475/Illustration-of-the-operation-of-the-SVM-algorithm-The-input-data-on-the-left-side.png\">\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Mona_Soliman_Habib/publication/267938227/figure/fig4/AS:669368325267460@1536601253122/SVM-Mapping-to-Higher-Dimension-Feature-Space.ppm\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de Costo\n",
    "\n",
    "Tomando como referencia el siguiente video:\n",
    "\n",
    "https://www.youtube.com/watch?v=geI6lM5iOl0&feature=youtu.be\n",
    "\n",
    "La función de costo del modelo es:\n",
    "\n",
    "$J(w,b) = \\frac{1}{2}w^{2}w + C\\sum_{i=1}^{m}max\\left\\lbrace0, 1 - t^{(i)}(w^Tx^{(i)}+b)\\right\\rbrace$\n",
    "\n",
    "donde:\n",
    "\n",
    "$t^{(i)} = \\left\\{ \\begin{array}{lcc}\n",
    "             1 &   si  & y^{(i)} = 1 \\\\\n",
    "             \\\\ -1 &  si & y^{(i)} = 0 \\\\\n",
    "             \\end{array}\n",
    "   \\right.$\n",
    "   \n",
    "$C$ funciona como un hyper-parámetro para el modelo, de forma que puede definirse aproximadamente como el inverso del factor de regularización $\\lambda$. \n",
    "\n",
    "Esta función de costo es de Tipo Bisagra o Hinge Loss. El objetivo de esta función es lograr la mayor separación entre clases. Se parte de dos rectas con valores 1 y -1.\n",
    "\n",
    "Tomando como referencia este video https://www.youtube.com/watch?v=VWwb3IAB6Rc&feature=youtu.be, la distancia entre ambas rectas se logra con este procedimiento general:\n",
    "\n",
    "* Evaluación de distancia entre las rectas y un punto (x,y).\n",
    "* Las rectas involucran los vectores de soporte\n",
    "* Se logra maximizar la distancia cuando el vector de parámetros $w$ es el menor.\n",
    "\n",
    "### Gradiente:\n",
    "\n",
    "* La función de costo se obtiene aplicando los multiplicadores de Lagrange.\n",
    "* Se deriva respecto a $w$ para obtener el gradiente.\n",
    "\n",
    "<img src=\"http://imgfz.com/i/QefEqGl.jpeg\">\n",
    "\n",
    "Luego se procede a la actualización de los parámetros de w:\n",
    "\n",
    "<img src=\"http://imgfz.com/i/6GbcCP0.jpeg\">\n",
    "<img src=\"http://imgfz.com/i/ABSpl2Q.jpeg\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de Hipotesis:\n",
    "\n",
    "Tomando como referencia el siguiente video:\n",
    "\n",
    "https://www.youtube.com/watch?v=geI6lM5iOl0&feature=youtu.be\n",
    "\n",
    "La función de hipotesis del modelo es:\n",
    "\n",
    "\n",
    "$y^{(i)} = \\left\\{ \\begin{array}{lcc}\n",
    "             1 &   si  & w^{T}x^{(i)} + b \\geq 0 \\\\\n",
    "             \\\\ 0 &  si & w^{T}x^{(i)} + b < 0 \\\\\n",
    "             \\end{array}\n",
    "   \\right.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diferencias entre Modelos\n",
    "\n",
    "Tomando como referencia los siguientes video:\n",
    "\n",
    "https://www.youtube.com/watch?v=geI6lM5iOl0&feature=youtu.be\n",
    "\n",
    "https://www.youtube.com/watch?v=xflpbarqi1I&feature=youtu.be\n",
    "\n",
    "### SVM vs Regresión\n",
    "\n",
    "* En general SVM se desempeña mejor que la regresión logística\n",
    "* La función de costo de regresión logística diverge más rápido que la de SVM, por lo que este último es más sensible a datos atípicos\n",
    "* Este no brinda una distribución de probabilidad, sino que clasifica de forma 0 y 1.\n",
    "\n",
    "### SVM vs KNN\n",
    "\n",
    "* SVM es un modelo paramétrico por lo que necesita entrenamiento, a diferencia de KNN\n",
    "* Para muchos puntos en poco espacio KNN es mejor opción que SVM\n",
    "* Para pocos puntos en mucho espacio SVM es mejor opción que KNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ventajas SVM\n",
    "\n",
    "* Si las clases son no linealmente separables es preferible usar SVM con un kernel no lineal.\n",
    "* Funciona bien si se trabaja en espacios de muchas dimensiones.\n",
    "* Puede trabajar en problemas de clasificación para datasets desbalanceados, es decir, donde hay una desproporcionalidad notoria entre el número de positivos y el número de negativos.\n",
    "* Los modelos de SVM generalizan bien por lo que el error por sesgo o bias es menor.\n",
    "* Es muy útil para dataset pequeños.\n",
    "\n",
    "## Desventajas SVM\n",
    "\n",
    "* La principal desventaja es que el proceso de entrenamiento suele ser ineficiente, por lo que no se recomienda para datasets de número alto.\n",
    "* Es complicado elegir una buena función de kernel.\n",
    "* No provee una distribución de probabilidad para las predicciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "* SVM funciona bien con poca data y datos desbalanceados\n",
    "* Puede realizarse clasificación no lineal empleando la técnica del kernel.\n",
    "* Tiende a producir menos overfitting\n",
    "* Tiene una buena generalización con nuevos datos cuando el modelo esta bien parametrizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "cancer = datasets.load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.3,random_state=109)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(kernel='linear') # Ejemplo usando un kernel lineal\n",
    "\n",
    "# Entrenamiento\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Prediccion\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9122807017543859\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(kernel='poly', degree = 6) # Ejemplo usando un kernel polinomial\n",
    "\n",
    "# Entrenamiento\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Prediccion\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9239766081871345\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(kernel='rbf') # Ejemplo usando un kernel rbf o gaussiano\n",
    "\n",
    "# Entrenamiento\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Prediccion\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.39766081871345027\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(kernel='sigmoid') # Ejemplo usando un kernel sigmoid\n",
    "\n",
    "# Entrenamiento\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Prediccion\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenarSVM(Xtrain, Ytrain, lr, C, epochs):\n",
    "    \n",
    "    m, n = Xtrain.shape\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, shape = (m, n), name = \"X\")\n",
    "    y = tf.placeholder(tf.float32, shape = (m, n), name = \"y\")\n",
    "    t = tf.placeholder(tf.float32, shape = (m, n), name = \"t\")\n",
    "\n",
    "    # Parametros\n",
    "    W = tf.Variable(tf.truncated_normal(shape = (n, 1), seed = 123), name = \"W\")\n",
    "    b = tf.Variable(tf.truncated_normal(shape = (1, 1), seed = 123), name = \"b\")\n",
    "        \n",
    "    # Modelo\n",
    "    with tf.name_scope(\"SVM_model\"):\n",
    "        svm = tf.add(tf.matmul(X, W), b, name = \"SVM\")\n",
    "        t_hat = tf.sign(svm, name = \"t_hat\")\n",
    "\n",
    "    # Función de costo\n",
    "    with tf.name_scope(\"FuncionCosto\"):\n",
    "        wN = tf.multiply(tf.constant(0.5), tf.reduce_sum(tf.square(W)), name = \"w_Norm\")\n",
    "        # Loss Hinge\n",
    "        classif = tf.reduce_sum(tf.maximum(tf.constant(0.), tf.subtract(tf.constant(1.), \n",
    "                                                                    tf.multiply(t, svm))), name = \"classif_term\")\n",
    "        # Función de costo \n",
    "        cost = tf.add(wN, tf.multiply(Cparametro, classif), name = \"Costo\")\n",
    "\n",
    "    # Gradient Descent Optimizer \n",
    "    with tf.name_scope(\"GradientDes.Optimizer\"):\n",
    "        train = tf.train.GradientDescentOptimizer(lr).minimize(cost) \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
